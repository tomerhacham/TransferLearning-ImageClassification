{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import PIL\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import AUTOTUNE\n",
    "from tensorflow.keras import layers\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 160, 160\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_and_images():\n",
    "    root = \".\\\\\"\n",
    "    imglabel_map = os.path.join(root, 'imagelabels.mat')\n",
    "    setid_map = os.path.join(root, 'setid.mat')\n",
    "    imagelabels = sio.loadmat(imglabel_map)['labels'][0]\n",
    "    setids = sio.loadmat(setid_map)\n",
    "    ids = np.concatenate([setids['trnid'][0], setids['valid'][0], setids['tstid'][0]])\n",
    "    labels, image_path = [] , []\n",
    "    for i in ids:\n",
    "        labels.append(int(imagelabels[i - 1]) - 1)\n",
    "        image_path.append(os.path.join(root, 'jpg', 'image_{:05d}.jpg'.format(i)))\n",
    "    return image_path, labels\n",
    "\n",
    "def split_data(ds, suffle=True,val_prop=0.25, test_prop=0.25):\n",
    "    if suffle:\n",
    "        ds = ds.shuffle(len(ds), reshuffle_each_iteration=False)\n",
    "\n",
    "    val_size = int(len(ds) * val_prop)\n",
    "    test_size = int(len(ds) * test_prop)\n",
    "    train_ds = list_ds.skip(val_size+test_size)\n",
    "    val_ds = list_ds.take(val_size)\n",
    "    test_ds = list_ds.take(test_size)\n",
    "    return train_ds,val_ds,test_ds\n",
    "\n",
    "def decode_img(img):\n",
    "    # Convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    # Resize the image to the desired size\n",
    "    return tf.image.resize(img, [img_height, img_width])\n",
    "\n",
    "def process_path(file_path, label):\n",
    "    # Load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label\n",
    "\n",
    "def prepare(ds, shuffle=False, augment=False):\n",
    "    resize_and_rescale = tf.keras.Sequential([\n",
    "        layers.Resizing(img_height, img_width),\n",
    "        layers.Rescaling(1. / 255)\n",
    "    ])\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        layers.RandomRotation(0.2),\n",
    "    ])\n",
    "\n",
    "    # Resize and rescale all datasets.\n",
    "    ds = ds.map(lambda x, y: (resize_and_rescale(x), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1000)\n",
    "\n",
    "    # Batch all datasets.\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    # Use data augmentation only on the training set.\n",
    "    if augment:\n",
    "        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    # Use buffered prefetching on all datasets.\n",
    "    return ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "image_path, labels = get_labels_and_images()\n",
    "list_ds = tf.data.Dataset.from_tensor_slices((image_path, labels))\n",
    "list_ds = list_ds.shuffle(len(list_ds), reshuffle_each_iteration=False)\n",
    "\n",
    "#Splitting the dataset\n",
    "train_ds,val_ds,test_ds = split_data(list_ds)\n",
    "\n",
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "train_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "train_ds = prepare(train_ds, shuffle=True, augment=True)\n",
    "val_ds = prepare(val_ds)\n",
    "test_ds = prepare(test_ds)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "320f6531f43e6fd8f1635c5566e7043d65993fafd620d58d41bf49c619f92797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
