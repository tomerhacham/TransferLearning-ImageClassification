{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import PIL\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import AUTOTUNE\n",
    "from tensorflow.keras import layers\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "root = 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_and_images():\n",
    "    imglabel_map = os.path.join(root, 'imagelabels.mat')\n",
    "    setid_map = os.path.join(root, 'setid.mat')\n",
    "    imagelabels = sio.loadmat(imglabel_map)['labels'][0]\n",
    "    setids = sio.loadmat(setid_map)\n",
    "    ids = np.concatenate([setids['trnid'][0], setids['valid'][0], setids['tstid'][0]])\n",
    "    labels, image_path = [] , []\n",
    "    for i in ids:\n",
    "        labels.append(int(imagelabels[i - 1]) - 1)\n",
    "        image_path.append(os.path.join(root, 'jpg', 'image_{:05d}.jpg'.format(i)))\n",
    "    return image_path, labels\n",
    "\n",
    "def split_data(ds, suffle=True,val_prop=0.25, test_prop=0.25):\n",
    "    if suffle:\n",
    "        ds = ds.shuffle(len(ds), reshuffle_each_iteration=False)\n",
    "\n",
    "    val_size = int(len(ds) * val_prop)\n",
    "    test_size = int(len(ds) * test_prop)\n",
    "    train_ds = list_ds.skip(val_size+test_size)\n",
    "    val_ds = list_ds.take(val_size)\n",
    "    test_ds = list_ds.take(test_size)\n",
    "    return train_ds,val_ds,test_ds\n",
    "\n",
    "def decode_img(img):\n",
    "    # Convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    # Resize the image to the desired size\n",
    "    return tf.image.resize(img, [img_height, img_width])\n",
    "\n",
    "def process_path(file_path, label):\n",
    "    # Load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label\n",
    "\n",
    "def prepare(ds, shuffle=False):\n",
    "    resize_and_rescale = tf.keras.Sequential([\n",
    "        layers.Resizing(img_height, img_width),\n",
    "        #layers.Rescaling(1. / 255)\n",
    "    ])\n",
    "    # Resize and rescale all datasets.\n",
    "    ds = ds.map(lambda x, y: (resize_and_rescale(x), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1000)\n",
    "\n",
    "    # Batch all datasets.\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    # Use buffered prefetching on all datasets.\n",
    "    return ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "image_path, labels = get_labels_and_images()\n",
    "list_ds = tf.data.Dataset.from_tensor_slices((image_path, labels))\n",
    "list_ds = list_ds.shuffle(len(list_ds), reshuffle_each_iteration=False)\n",
    "\n",
    "#Splitting the dataset\n",
    "train_ds,val_ds,test_ds = split_data(list_ds)\n",
    "\n",
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "train_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "train_ds = prepare(train_ds, shuffle=True)\n",
    "val_ds = prepare(val_ds)\n",
    "test_ds = prepare(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "IMG_SIZE = (img_height, img_width)\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "\n",
    "# Freeze the base_model\n",
    "base_model.trainable = False\n",
    "\n",
    "image_batch, label_batch = next(iter(train_ds))\n",
    "feature_batch = base_model(image_batch)\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "\n",
    "prediction_layer = tf.keras.layers.Dense(len(set(labels)))\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "# Data Augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomZoom(height_factor=(-0.3, 0.3)),\n",
    "    layers.RandomRotation((-0.3,0.3)),\n",
    "    layers.RandomContrast((0.1, 0.9)),\n",
    "    layers.RandomCrop(img_height, img_width),\n",
    "    layers.Lambda(lambda x:tf.clip_by_value(x,0,255))\n",
    "])\n",
    "\n",
    "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 95/128 [=====================>........] - ETA: 1:20 - loss: 4.7549 - accuracy: 0.0243"
     ]
    }
   ],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "history = model.fit(train_ds,\n",
    "                validation_data=val_ds,\n",
    "                epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "result = model.evaluate(test_ds)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "320f6531f43e6fd8f1635c5566e7043d65993fafd620d58d41bf49c619f92797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
